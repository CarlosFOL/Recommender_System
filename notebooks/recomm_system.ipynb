{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e37599-5e5e-42e6-8423-85b6de214733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_project = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if path_project not in set(sys.path):\n",
    "    sys.path.append(path_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcaa6ed0-ff2c-479a-892d-54fd37794bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlosfol/Desktop/Git_Projects/recommender_system/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from scipy.sparse import csr_matrix\n",
    "from scripts.als import ALS\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5f7a5-d8db-4a08-9f05-7c6be3254dce",
   "metadata": {},
   "source": [
    "# Recommender AI System\n",
    "\n",
    "This notebook shows all the steps that we will take to build our machine learning model. In this case, we decided to use the library `Polars` for manipulating the training dataset, because this has a better performance when working with large datasets compared to `Pandas`. This is a big help for us, because we don't have the best hardware for this task, so the most efficient tools are our friends.    \n",
    "\n",
    "Basically, the goal is to build a recommender system that is capable to recommend 5 different products to a customer by checking its purchase history. The columns of the training dataset are the following:\n",
    "* Session ID\n",
    "* Date of interaction\n",
    "* Interaction timestamp\n",
    "* User ID\n",
    "* User's country\n",
    "* Partnumber: The id of the product with which the interaction occurred.\n",
    "* device_type: Type of device used.\n",
    "* pagetype: Type of page where the interaction occurred within the e-commerce site.\n",
    "* add_to_cart: Boolean indicating if the interaction was adding to the card.\n",
    "\n",
    "The approach to create a recommender system is based on **collaborative filtering**. This technique consists of making assumptions about a user based on the information from other users who have similar behaviors when purchasing or interacting with items. This data can reveal **latent patterns** that represent both the preferences of clients and the characteristics of the products/items (Hu, Y. et all, 2008, p. 1).\n",
    "\n",
    "To apply this concept mathematically, we need to use matrix factorization, specifically **ALS (Alternating Least Squares)**. First, we have to create a matrix of interactions between products and clients. We face the challenge that there are null values in the user_id field, but this information is really important because it reveals which products are viewed together, add-to-cart patterns, and other valuable behavioral insights. Therefore, we have decided to include these records (85% of the data) by using the `session_id` as a pseudo-user identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0649ec9-532d-45d2-8d41-39e86bef3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train_data: pl.LazyFrame = pl.scan_parquet(\"../data/train.parquet\") # using Lazy API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99f8a0-0054-4621-b856-52b46fb139a6",
   "metadata": {},
   "source": [
    "We will assign a pseudo-user to those record that lack a user id based on their session id's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4042c1-a544-4294-8698-d8657584329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expression to modify the user_id\n",
    "unified_user= (\n",
    "    pl.when( pl.col('user_id').is_not_null() )\n",
    "        .then( pl.concat_str( [pl.lit(\"u_\"), pl.col(\"user_id\").cast(pl.Utf8)] ) )\n",
    "        .otherwise( pl.concat_str( [pl.lit(\"s_\"), pl.col(\"session_id\").cast(pl.Utf8) ]) )\n",
    "        .alias(\"unified_user_id\")\n",
    ")\n",
    "\n",
    "# Batch processing:\n",
    "CHUNK = 1_000_000\n",
    "N: int = df_train_data.select( pl.len() ).collect().item() # Nrows\n",
    "\n",
    "df_unified : pl.LazyFrame = pl.concat(\n",
    "    df_train_data.slice(offset, CHUNK).with_columns(unified_user).collect()\n",
    "    for offset in range(0, N, CHUNK)\n",
    ").lazy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aaf7b-52a6-4898-9d2f-a76318a77289",
   "metadata": {},
   "source": [
    "The problem is that this interaction matrix is sparse, because it is most likely that a client doesn't interact with all the products in the catalog. Therefore, a solution is to decompose it into two dense matrices: $X$ and $Y$:\n",
    "\n",
    "$$R_{m \\times n} = X_{m \\times f} \\times Y_{f \\times n}^T$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $R \\rightarrow$ Interaction matrix (users $\\times$ items)\n",
    "* $X$ and $Y$ are the dense matrices, where $X$ represents user latent factors and $Y$ represents item latent factors\n",
    "* $f \\rightarrow$ Represents the number of latent factors, whose value is much lower than $m$ and $n$\n",
    "\n",
    "---\n",
    "\n",
    "In e-commerce, we rarely have explicit ratings. We instead have implicit feedback, meaning the frequency of actions that suggest interest but don't directly tell us how much a user likes something (Hu, Y. et all, 2008, p. 2). You can think of it this way:\n",
    "\n",
    "* View a product page (mild interest)\n",
    "* View it multiple times (stronger interest)\n",
    "* Add it to cart (very strong interest)\n",
    "* Purchase it (maximum interest)\n",
    "\n",
    "To translate these actions to numbers in order to ALS algorithm can understand, we use the something called *confidence*. They will represent the values in the table of interactions and the way to compute them as follows:$$c_{ui} = 1 + \\text{view count} + 10\\cdot\\text{cart count}$$\n",
    "Where:\n",
    "* View count: How many times the user $u$ has seen the product or item $i$.\n",
    "* Cart count: How many times the user $u$ has added the product $i$ to a cart. This signal is more stronger than a view, so it's weighted 10 times more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e9fdc09-5911-40f5-a7b2-a3421566cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the view and car count for each pair (user, product):\n",
    "df_interactions = df_unified.group_by([\"unified_user_id\", \"partnumber\"]).agg([\n",
    "            pl.len().alias(\"view_count\"),\n",
    "            pl.col(\"add_to_cart\").sum().alias(\"cart_count\").fill_null(0)\n",
    "])\n",
    "\n",
    "# Calculate confidence scores (implicit feedback)\n",
    "df_interactions = df_interactions.with_columns([\n",
    "    (1 + pl.col(\"view_count\") + 10 * pl.col(\"cart_count\")).alias(\"confidence\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5852c5-3087-4379-b1e0-1aa6a6a92dbf",
   "metadata": {},
   "source": [
    "Once calculated the confidence values, we can create the sparse matrix $R$ (`df_interactions`) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27a27d1d-be10-4c19-9d84-cf3e61000656",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping = (\n",
    "    df_interactions.select(\"unified_user_id\")\n",
    "                    .unique()\n",
    "                    .sort(\"unified_user_id\")\n",
    "                    .with_row_index(\"user_id\")\n",
    ")\n",
    "\n",
    "item_mapping = (\n",
    "    df_interactions.select('partnumber')\n",
    "                    .unique()\n",
    "                    .sort('partnumber')\n",
    "                    .with_row_index('item_id')\n",
    ")\n",
    "\n",
    "# Join to get indices\n",
    "df_interactions = (\n",
    "    df_interactions\n",
    "    .join(user_mapping, on='unified_user_id')\n",
    "    .join(item_mapping, on='partnumber')\n",
    ")\n",
    "\n",
    "# Sparse matrix\n",
    "csr_interactions = csr_matrix(\n",
    "    ( # (data, (row_idx, col_idx))\n",
    "        df_interactions[\"confidence\"].to_numpy(),\n",
    "        ( df_interactions[\"user_id\"].to_numpy(),\n",
    "          df_interactions[\"item_id\"].to_numpy() ) \n",
    "    ), shape = (len(user_mapping), len(item_mapping))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88b0a7-d4d6-429c-981d-bb6f4c63c8ca",
   "metadata": {},
   "source": [
    "It's important to understand the loss function that we are trying to minimize when using the algoritm ALS:\n",
    "\n",
    "$$L = \\sum_{(u,i) \\in \\Omega} c_{ui}(r_{ui} - x_u^T y_i)^2 + \\lambda\\left(\\sum_u ||x_u||^2 + \\sum_i ||y_i||^2\\right)$$\n",
    "\n",
    "Think of this equation as a way to measure \"how wrong\" our recommendations are, and our goal is to make this number as small as possible. Let's understand each part:\n",
    "\n",
    "* $r_{ui} - x_u^T y_i$ represents the difference between what actually happened (did the user interact with the item?) and what our model predicts.\n",
    "* Think of $\\hat{p}_{ui} = x_u^T y_i$ as a \"compatibility score\" (or preference) between user $u$ and item $i$. The \"magic\" behind the dot product is that is mathematical way of measuring similarity - the higher the number, the better the match:$$x\\cdot y^t = |x||y|\\cos(\\theta) $$\n",
    "* $c_{ui}$ acts like a \"trust multiplier\" - some interactions are more reliable than others.\n",
    "* $\\lambda\\left(\\sum_u ||x_u||^2 + \\sum_i ||y_i||^2\\right)$ prevents the model from becoming too complex.\n",
    "\n",
    "\n",
    "\n",
    "<u>Parameters explanation:</u>\n",
    "\n",
    "* $c_{ui}$: Confidence level of user $u$ for product/item $i$. This is crucial for an implicit feedback system.\n",
    "* $r_{ui}$: The input data associate between user $u$ and item $i$.\n",
    "* $x_u$: Latent factor vector representing user $u$\n",
    "* $y_i$: Latent factor vector representing item $i$\n",
    "* $\\lambda$: Regularization parameter to avoid overfitting.\n",
    "* $\\Omega$: Set of observed interactions between users and products.\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "You may be wondering how this algorithm works. Let's explain:\n",
    "\n",
    "1. Initialization: $X$ and $Y$ are initialized randomly. We also set the key parameters that define the training stage: confidence multiplier ($\\alpha$), regularization ($\\lambda$), and number of factors ($f$).\n",
    "2. Fix $Y$, solve for $X$: Due to ALS nature, you can only optimize one matrix at a time, keeping the other one fixed. For each user, we solve the following equation:\n",
    "$$x_u = (Y^T C^u Y + \\lambda I)^{-1} Y^T C^u r^u$$\n",
    "3. Fix $X$, solve for $Y$: The process is the same as step 2, but now we optimize the item factors while keeping user factors fixed.\n",
    "4. Repeat: Continue alternating between steps 2 and 3 until the algorithm converges or the maximum number of iterations is reached. Each step (i.e. recomputing user-factors and item-factors) is guaranteed to lower the value of the cost function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<u>Key Parameters and their impact:</u>\n",
    "\n",
    "According to Hu Y. et all (2008):\n",
    "\n",
    "* Number of Factors ($f$): This determines the dimensionality of the latent space. Think of it as the number of hidden \"concepts\" or \"topics\" that you want to capture about user preferences and item characteristics.\n",
    "    * Sweet spot: 20-200 for large volumes of data\n",
    "\n",
    "* Regularization Parameter ($\\lambda$): Controls how much we penalize large factor values to prevent overfitting.\n",
    "\n",
    "    * Sweet spot: 0.1-1.0\n",
    "\n",
    "* Confidence Multiplier ($\\alpha$): Determines how much more we trust frequent interactions compared to single interactions. We won't use it, because the we have defined the way to calculate the confidence values.\n",
    "\n",
    "    * Typical range: 1-40\n",
    "\n",
    "* Number of Iterations: The balance between convergence and overfitting.\n",
    "\n",
    "    * Typical range: 15-30\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "### Training the model\n",
    "\n",
    "* Use different number of factors.\n",
    "* Better results are achieved when the model is regularized (Use different lambdas)\n",
    "\n",
    "We will use different settings to determine the model with the best performance during the validation stage. We will increase the number of latent factors to see how the model behaves, check whether the performance gets worse when relaxing the regularization parameter. We will also increase our confidence in frequent interactions and try with different number of iterations to study how the model behaves based on this variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2b06d-bc6c-4eb4-b249-df868c8dcb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "param_grid = {\n",
    "    \"factors\": np.array([20, 70, 150]),\n",
    "    \"regularization\": np.array([0.1, 0.55, 1]),\n",
    "    \"iterations\": np.array([15, 20, 30])}\n",
    "\n",
    "# json file provided by INDITEX for performing the validation stage.\n",
    "with open(\"../data/example_predictions_3.json\") as f:\n",
    "    predictions = json.load(f)\n",
    "predictions = predictions[\"target\"]\n",
    "\n",
    "# Let's start training the models\n",
    "models = [None]*3\n",
    "i = 0\n",
    "for setting in itertools.product(param_grid.values()):\n",
    "    params = dict( zip(param_grid.keys(), setting) )\n",
    "    model = ALS(**params)\n",
    "    model = model.train(csr_interactions)\n",
    "    models[i] = model\n",
    "    i += 1\n",
    "\n",
    "# Validation stage\n",
    "accuracies = [None]*3\n",
    "# The outputs of the models have to be translated to real items ids to measure their accuracies.\n",
    "translate_item_id = lambda item_id: (\n",
    "    item_mapping\n",
    "        .filter(plt.col(\"item_id\") == item_id)\n",
    "        .select(\"partnumber\")\n",
    "        .collect()\n",
    "        .item()\n",
    ") )\n",
    "\n",
    "i = 0\n",
    "for user_id, items_id in predictions.items():\n",
    "    items_id = set(items_id) # No matter the order\n",
    "\n",
    "    # Translate the user_id by using the user_mapping\n",
    "    user_id = (\n",
    "        user_mapping\n",
    "            .filter(pl.col(\"unified_user_id\") == f\"u_{user_id}\") \n",
    "            .select(\"user_id\")\n",
    "            .collect()\n",
    "            .item()\n",
    "    )\n",
    "\n",
    "    # Get the outputs\n",
    "    outputs = models[i].recommend(user_id, csr_interactions[user_id], N=5, filter_already_liked_items=True)\n",
    "    # It's necessary to mapping to original item ids\n",
    "    items_outputs = {translate_item_id(item_id) for item_id in outputs[0]}\n",
    "\n",
    "    accuracies[i] = len(items_id & items_outputs) / 5\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Performance Summary:\n",
    "for i in range(3):\n",
    "    print(f\"Accuracy of model {i}: {accuracies[i]: .2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3461e060-2284-464c-875a-49e5dc1cebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 15/15 [00:00<00:00, 55.16it/s, loss=0.000606]\n"
     ]
    }
   ],
   "source": [
    "als = ALS()\n",
    "model = model.train(csr_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6183808b-9c28-4959-8181-642b2ac42dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([614, 358, 736, 451, 715], dtype=int32),\n",
       " array([0.00386002, 0.00258447, 0.00251338, 0.00250605, 0.00224712],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You have to use the id used in csr matrix.\n",
    "model.recommend(85, csr_interactions[85], N=5, filter_already_liked_items=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4b993-417a-4e5c-9e9c-7f419089161c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "* Hu, Y., Koren, Y., & Volinsky, C. (2008). Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE International Conference on Data Mining (pp. 263-272)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
