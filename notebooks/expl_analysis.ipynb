{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461966e4-1aa3-4b1b-a54d-e7fdf58d1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that the interpreter can access scripts.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "path_project = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if path_project not in set(sys.path):\n",
    "    sys.path.append(path_project) # To avoid insert it many times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3719c67b-f5fe-4169-9f06-f0edfe1adf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scripts.dbscan import DBSCAN_Analysis\n",
    "from scripts.kmeans import centroid_clusters, plot_kmeans_stats, kmeans_statistics \n",
    "import seaborn as sns\n",
    "from time import time_ns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457ef5c-e156-4702-b0d8-189d79f6d819",
   "metadata": {},
   "source": [
    "# <span style = \"color:brown\"><ins>ANALYSIS OF CLIENT'S BEHAVIOUR OF A E-COMMERCE RETAIL</ins></span>\n",
    "\n",
    "The datasets that we will use for this exploratory analysis are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f19b8-28cb-4a10-af31-0cb588570d04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## USERS DATASET\n",
    "\n",
    "### Variables:\n",
    "\n",
    "* id: User's identifier\n",
    "* country: Where the user belongs.\n",
    "\n",
    "[Investopedia (2024, August 14) What is Frequency, Recency, Monetary value (RFM) in Marketing?](https://www.investopedia.com/terms/r/rfm-recency-frequency-monetary-value.asp)\n",
    "\n",
    "The RFM is a marketing tool used for a firm to identify which are the best clients or customers according by their spending habits. Through this model, an organization is able to score them (1 to 5) by using these variables. And in this way, they can predict which ones are likely to purchase their products again or how to turn occasional buyers into habitual ones.\n",
    "\n",
    "RFM analysis allows a comparison between potential contributors and clients. It gives organizations a sense of how much revenue comes from repeat customers (vs new customers).\n",
    "\n",
    "* R: Recency. How long ago the user made a purchase.\n",
    "* F: User's purchase frequency. How often they make purchases.\n",
    "* M: Monetary value that the customer spends on purchases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51317f46-ea1a-4de6-a527-9a0ede21125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION OF THE RAW DATA:\n",
    "df_users = pd.read_csv(\"../data/users.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabeda67-5982-48cd-90f5-845a34dbcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784d5be-eb4f-4160-95c8-4b389989dc6a",
   "metadata": {},
   "source": [
    "Let's see which countries's id are the most frequent in the users of this retail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59501e-b78f-4aca-a6b1-c908bf69f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_users.shape[0]\n",
    "# Plot the proportion of users by country's id\n",
    "prop = df_users[\"country\"].value_counts() / n\n",
    "prop[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d43d6-ce33-4c1d-aafc-58cce36b45d9",
   "metadata": {},
   "source": [
    "We can see that the $98$% of the users belongs to the country whose identifier is 25. Unfortunately, we don't have a table that tell us which country is according to the id.\n",
    "\n",
    "On the other hand, it's interessant to take a look of the last time that users has made a purchase in the e-commerce retail. We will consider the values of `R` is expresseed in days according to its description for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9aaa0-c2df-40a4-b06f-e6bf86911a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users['R'].plot(kind = \"hist\",\n",
    "                   title = \"Distribution of Users's recency\",\n",
    "                   density = True,\n",
    "                   ylabel = \"Density\",\n",
    "                   xlabel = \"R\",\n",
    "                   figsize = (6, 3)\n",
    "                  )\n",
    "df_users['R'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf973c3-246f-4a27-a460-f0b69f963651",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_pct = lambda threshold: np.mean(df_users[\"R\"] <= threshold).round(2) * 100\n",
    "R_outcome = lambda days: print(f\"The {R_pct(days)}% of users has made a purchase in the last {days//30} month(s) aprox.\")\n",
    "# Compute how long ago the vast majority of users has made a purchase\n",
    "R_outcome(200)\n",
    "# The pct of people that made a purchase in the last month\n",
    "R_outcome(30)\n",
    "# Customers who have not made a purchase in over year\n",
    "R_geq_365 = 100 - R_pct(365)\n",
    "print(f\"The {R_geq_365}% of users have not made a purchase in over a year.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c15de-d09b-4365-a3a2-c097768727b4",
   "metadata": {},
   "source": [
    "This variable has a high variability. We can see that more than 50% of records in this sample is from people that has made a purchase in the last month. The organization needs to pay attention of them, because the more recently a user has made a purchase with a company, the more likely they will continue to keep the business and brand in mind for subsequent purchases.\n",
    "\n",
    "Now, let's know about how often the clients make a purchase by studying the variable `F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726008e-b86a-4bfd-be7a-e82867598266",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_intervals = pd.cut(df_users['F'], \n",
    "                     [0, 1000, df_users['F'].max()], \n",
    "                     labels = [\"<1k\", \">1k\"],\n",
    "                     include_lowest=True)\n",
    "# Graph\n",
    "fig, ax = plt.subplots(figsize = (4, 3))\n",
    "ax.pie(F_intervals.value_counts(),\n",
    "        autopct = \"%.2f%%\",\n",
    "        labels = F_intervals.unique())\n",
    "ax.set_title(\"Clients who have made more than 1k purchases\\nVS those who haven't\")\n",
    "plt.show()\n",
    "# Stats\n",
    "df_users['F'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596cbf8c-481f-4d8e-8a05-38319c75896e",
   "metadata": {},
   "source": [
    "It's amazing that there are customers that have made more than 1k purchases, let's see when it was the last time that they have made a purchase in order to see if this happened in the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447d904-c208-4e59-a604-66926ca3e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_geq_1k = df_users[\"F\"] >= 10**3\n",
    "# Graph\n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.set_title(\"Recency from customer\\nwith more than 1k purchases\")\n",
    "sns.ecdfplot(df_users.loc[F_geq_1k, :], x=\"R\", ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98802ba9-0e1d-4887-8282-a6fb6fea6071",
   "metadata": {},
   "source": [
    "As we have expected, more than 80% of these customers have made a purchase in at most 20 days ago. They could be the group of loyal customers for this organization due to the combination of their recency (`R`) and frequency (`F`).\n",
    "\n",
    "It's time to visualize the amount of money that the customers have spent in this period. We want to know if those who have make a large number of purchases are the ones who have spent more money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ef769-3fa3-4992-aef4-98cdd2588edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users[\"M\"].plot(kind = \"box\", \n",
    "                  figsize = (5, 3),\n",
    "                  title = \"Monetary value that customers spent in purchases\")\n",
    "df_users[\"M\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005324d-2449-4e19-b119-c6cfec7d0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_leq_1c = np.mean(df_users[\"M\"] <= 100) * 100\n",
    "print(f\"The {M_leq_1c: .2f}% of customer have spent at most 100 dollars in purchases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45072692-cf7f-45cc-93c7-0614b45b8d21",
   "metadata": {},
   "source": [
    "There is people who have spent millions of dollars in purchases! Let's check if they are those who have made a purchase in the last month and have a large number of purchases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a019-fd26-4ce8-9a51-01b225eb08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_geq_1M = df_users.loc[df_users[\"M\"] >= 1e6, [\"R\", \"F\", \"M\"]] \n",
    "# Graph\n",
    "fig, axes = plt.subplots(1, 3, figsize = (10, 2))\n",
    "fig.suptitle(\"RFM analysis of customers that have spent > 1 million dollars in purchases\")\n",
    "for idx, stat in enumerate(M_geq_1M.columns):\n",
    "    sns.boxplot(M_geq_1M[stat], ax=axes[idx], orient = \"h\")\n",
    "# Stats\n",
    "M_geq_1M.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9603c-b800-4c93-b7ce-2963cf23cbf8",
   "metadata": {},
   "source": [
    "It's interesting that despite the fact these customers are those who spent the largest amount of money in this e-commerce retail, there is a high variability in `R`, where the 50% of people have not made a purchase for more than 10 months (>300 days) ago approx. Also, they are not frequent customers, because the 50% of customers have only buy 1 stuff, but its cost is millionaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a64a30-5308-4628-a233-81dc70d4f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_revenue = np.round(M_geq_1M.loc[:, \"M\"].sum() / df_users.loc[:, \"M\"].sum(), 2) * 100\n",
    "print(f\"Although these customers are not the active buyers, their contribution represent the {pct_revenue: .2f}% of revenues for this organization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b129de-4745-4e60-a11d-92bdc835daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_active_buyers = df_users.loc[F_geq_1k, :]\n",
    "# Graph\n",
    "fig, ax = plt.subplots(figsize = (6, 3))\n",
    "ax.set_title(\"Monetary value spent by active buyers\")\n",
    "ax.set_xlabel(\"Monetary value ($)\")\n",
    "sns.stripplot(M_active_buyers, x = \"M\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86e072-8595-4f73-a6db-d47cb69f9ef3",
   "metadata": {},
   "source": [
    "The active buyers are also the ones who purchase the cheapest things. Because they have made more than 1k purchases and the vast majority of these observations has only spent at most 200$ dollars.\n",
    "\n",
    "Through these results one could think that there is no a relation between `F` and `M`. Because, if a customer spend a lot of money, that does not mean that this is a active buyer in the e-commererce retail, and vicerversa. Even though, a possible relation could exist between the frequency and recency, after analyzing the group of customers that have made more than 1k purchases. Let's see the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce2dc5-1b4b-40f9-9727-5d5040b22fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's necessary to choose a sampple from our dataset due to its size \n",
    "np.random.seed(123)\n",
    "\n",
    "start = time_ns()\n",
    "\n",
    "# Graph\n",
    "(\n",
    "    sns.pairplot(\n",
    "        data= df_users.loc[\n",
    "            np.random.choice(range(1, n + 1), size = 5000, replace = False),\n",
    "            ['R', 'F', 'M']],\n",
    "        corner=True,\n",
    "        height=2)\n",
    ")\n",
    "plt.plot()\n",
    "\n",
    "end = time_ns() - start\n",
    "print(f\"This taks tooks {end * pow(10, -9): .2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbf0e4-9c5e-4a3f-8c53-bacdfa6e32a6",
   "metadata": {},
   "source": [
    "You can see through the scatterplot between `F` and `M` that there is no a correlation. People who makes a lot of purchases are not the ones that more money spent. Also, as we have said, the people who have peformed millionaire purchases have the lowest `F` values.  \n",
    "\n",
    "On the other hand, despite the fact that there is no clear correlation between `F` and `R`, most customers who made their last purchase recently are the ones more purchases have made.\n",
    "\n",
    "It would be interesting to use the $k$-means algorithm in order to create clusters of customers and analyze what kind of attributes are significant in each of them. To choose the correct value of $k$, we will to analyze the following statistics:\n",
    "\n",
    "* Within Cluster Sums of Squares or Inertia: To measure how compact the clusters are. $$\\text{WSS} = \\Sigma_{i = i}^{N_c}\\Sigma_{x \\in C_i}\\|x - \\bar{X}_{C_i}\\|^2$$\n",
    "\n",
    "* Between Cluster Sums of Squares: To measure how well-separated the clusters are. $$\\text{BSS} = \\Sigma_{i = 1}^{N_c} |C_i|\\cdot\\|\\bar{x}_{C_i} - \\bar{X}\\|^2$$\n",
    "\n",
    "Where: $$N_c\\rightarrow\\text{Number of clusters};\\quad|C_i|\\rightarrow\\text{Number of elements of ith cluster};\\quad\\bar{X}_{C_i}\\rightarrow\\text{Centroid of ith cluster};\\quad\\bar{X}\\rightarrow\\text{Overall centroid}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1973a87-51b5-43c6-8abb-5d5899592337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This task is computanionally intensive. So we decide to store the results, to avoid\n",
    "# executing this cell many times:\n",
    "\n",
    "data_path = os.path.abspath(os.path.join(os.getcwd(), \"../data\"))\n",
    "files = set(os.listdir(data_path))\n",
    "# Training data\n",
    "np.random.seed(123)\n",
    "sample = np.random.choice(range(1, n + 1),\n",
    "                          size = round(n*0.2),\n",
    "                          replace = False)\n",
    "X_train = df_users.loc[sample, [\"R\", \"F\", \"M\"]]\n",
    "if \"kmeans.csv\" not in files:\n",
    "    # Set training data\n",
    "    start = time_ns()\n",
    "    \n",
    "    WSS, BSS = kmeans_statistics(X_train)\n",
    "    kmeans_data = pd.DataFrame({\"WSS\": WSS, \"BSS\": BSS})\n",
    "    kmeans_data.to_csv(\"../data/kmeans.csv\")\n",
    "\n",
    "    end = (time_ns() - start) * pow(10, -9)\n",
    "    print(f\"This task tooks {end: .2f} seconds\")\n",
    "else:\n",
    "    print(\"The results have been stored in ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55b4ac-ff67-4709-a483-07d7f68a029c",
   "metadata": {},
   "source": [
    "Let's apply the elbow method to find out the $k$ that increases the BSS and decreases the WSS significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9d3fe-6ecc-41bf-bd70-c0c960bfc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results of k-means:\n",
    "df_kmeans = pd.read_csv(\"../data/kmeans.csv\")\n",
    "df_kmeans.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n",
    "df_kmeans[\"K\"] = range(2, df_kmeans.shape[0] + 2)\n",
    "# Graph\n",
    "plot_kmeans_stats(df_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2defb-e2b7-4000-87b9-15b67775244e",
   "metadata": {},
   "source": [
    "You can see that there is a $k$ between 0 and 10 (approx.), where both WSS and BSS converge. Let's take a closer look at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7866135-cae1-4d74-944d-86887a9539c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_stats(df_kmeans.iloc[:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906f734-93d2-40df-b3dd-d05f8e73ec60",
   "metadata": {},
   "source": [
    "The elbow is at $k = 4$. And we have only considered the 20% of observations for the training data. So, let's check which features are characterized for these 4 clusters obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e90b8f-bd03-4884-b4fc-34e3fbe56160",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=4, random_state=123)\n",
    "model.fit(X_train)\n",
    "# Get centroids\n",
    "centroid_clusters(model, X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c416485-1b8e-4917-9987-69d5db2f9cef",
   "metadata": {},
   "source": [
    "As we saw earlier, only a small group of individuals ($\\approx <5\\%$) have spent millions of dollars in purchases, despite the fact that they are not frequent customers. On the other hand, since the $\\approx 97\\%$ of people have spent at most as 100 dollars in purchases, then it is normal that when sampling, the vast majority of data points will come from this group.\n",
    "\n",
    "Although the $K$-means algorithm by using the elbow method suggest to take $k=4$. We could consider that there are two groups that are defined in the following way:\n",
    "* Cluster 1: People who could be considered frequent shoppers. Well, on average, they have purchased more than 35 products and their most recent purchase was in the last 3 months.\n",
    "* Cluster 2: There are characteristics that people who have spent millions of dollars on purchases have in common. On average, they have only purchased less than 5 products and their last purchases was more than 10 months ago."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d800944b-303b-4a76-8d75-26f46262d562",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PRODUCTS DATASET\n",
    "\n",
    "It contains the information about the products that digital retail offers:\n",
    "\n",
    "### Variables:\n",
    "* Discount: Indicate if the product is on sale (eg. 1 or 0).\n",
    "* Embedding: A lower dimensional representation of the product flat's image from computer vision techniques.\n",
    "* Partnumber: Product ID.\n",
    "* Color ID: Product color identifier.\n",
    "* Code section: Section to which the product belongs.\n",
    "* Family: Product family to which the product belongs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9823cc-7093-4e0a-ba07-2a6a3aabab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/products.pkl\", \"rb\") as df:\n",
    "    df_products = pickle.load(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad5de1-294d-477a-a0bc-abb3e85cc621",
   "metadata": {},
   "source": [
    "We have the embeddings of the products's images, which are multidimentional vectors that capture the main visual features of each product. So, by using this variable, we can assess the similarity between products, analyzing their shapes, textures and other characteristics. \n",
    "\n",
    "So, the goal is to group those products with similar features by using the HDBSCAN algorithm (Campello et.all, 1996) and compare this result by using the $k$-means algorithm. Also, we could suspect about the number of clusters by using the `Family` or `Code section` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be06a65-17fa-4b0a-bb0e-96def8e86c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cod_section</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cod_section  family\n",
       "min          1.0     1.0\n",
       "max          4.0   217.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.loc[:, [\"cod_section\", \"family\"]].describe().loc[[\"min\", \"max\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8194055f-bc0f-484a-96e7-067ec427c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43692 entries, 0 to 43691\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   discount     43692 non-null  object \n",
      " 1   embedding    37522 non-null  object \n",
      " 2   partnumber   43692 non-null  int32  \n",
      " 3   color_id     43692 non-null  int32  \n",
      " 4   cod_section  43602 non-null  float64\n",
      " 5   family       43692 non-null  int32  \n",
      "dtypes: float64(1), int32(3), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_products.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c5deb-a031-41d2-b42f-aaec42489068",
   "metadata": {},
   "source": [
    "There is a problem with the column `embedding`, since there are null values into it. So, we have to discard them to apply the ML algorithms. Also, it's important to normalize them in order to ignore their magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2500bf3-5e2d-4e27-a693-c9191b619d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.csv already exists\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "data_files = os.listdir(os.path.abspath(os.path.join(os.getcwd(), \"../data\")))\n",
    "if \"embeddings.csv\" not in data_files:\n",
    "    embeddings = df_products.loc[df_products.embedding.apply(lambda x: x is not None), \"embedding\"]\n",
    "    embeddings = embeddings / embeddings.apply(lambda emb: np.linalg.norm(emb))\n",
    "    df_embeddings = []\n",
    "    for record in embeddings:\n",
    "        df_embeddings.append(record)\n",
    "    pd.DataFrame(df_embeddings).to_csv(\"../data/embeddings.csv\")\n",
    "else:\n",
    "    print(\"embeddings.csv already exists\")\n",
    "    df_embeddings = pd.read_csv(\"../data/embeddings.csv\").iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d527794-2553-4599-8120-f5c68d77970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random sample\n",
    "np.random.seed(987)\n",
    "n = df_embeddings.shape[0]\n",
    "products_sample = df_embeddings.iloc[np.random.choice(n + 1, round(n * 0.2)), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52983d-9284-4ae7-bb4e-a1b2b877539e",
   "metadata": {},
   "source": [
    "We have decided to use HDBSCAN instead of classical DBSCAN, because the embeddings are high dimensional (d = 1280), what reduces the efectiveness of this algorithm due to the curse of dimensionality, where as dimensions increase, the contrast between near and far distances diminishes. This causes that the task of finding the hyperparameter `eps`, maximum distance between two points to be considered neighbors, mathematically intractable. \n",
    "\n",
    "On the other hand, with HDBSCAN we only need to set the minimum size of a cluster, value that we can approximate by using the smallest frequency within our categorical variables (`cod_section`, `family`). This hyperparameter is similar to `min_samples`, minimum number of neighbors around a point $p$ to be considered a core point and to form a dense region, in DBSCAN.\n",
    "\n",
    "Also, we will use the cosine distance $\\Big(1 - \\cos(\\theta)\\Big)$ for this task, because the direction between these multi-dimentional vectors tell us which products have similar visual features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df17c7cc-4479-4bd2-bf29-fd653b10f9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cod_section</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1870.500000</td>\n",
       "      <td>38.482051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>892.006166</td>\n",
       "      <td>98.963668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1071.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1455.750000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1632.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2046.750000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3147.000000</td>\n",
       "      <td>825.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cod_section      family\n",
       "count     4.000000  195.000000\n",
       "mean   1870.500000   38.482051\n",
       "std     892.006166   98.963668\n",
       "min    1071.000000    1.000000\n",
       "25%    1455.750000    5.000000\n",
       "50%    1632.000000   11.000000\n",
       "75%    2046.750000   29.000000\n",
       "max    3147.000000  825.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_products.loc[\n",
    "        products_sample.index, \n",
    "        [\"cod_section\", \"family\"]].\n",
    "    apply(lambda var: \n",
    "          var.value_counts().describe())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4dc905-dd01-4235-b2ca-fc1de254df72",
   "metadata": {},
   "source": [
    "Desbalance con respecto al tamaño mínimo de un clúster a considerar basado en nuestras dos variables categóricas, donde para code section el más peqeuño es de 1071 elementos y el de family es de 1 (absurdo).\n",
    "\n",
    "Esto sucede porque hay productos que pueden pertenecer a una misma sección, pero esta puede estar compuesta por diversas familias (heatmap?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa05a1cf-5d11-4c49-8ae8-1decb3360fab",
   "metadata": {},
   "source": [
    "Tomar diferentes pares de hiperparámetros y medir sus estadísticos principales:\n",
    "\n",
    "* Stats table:\n",
    "  - [x] Minimum size of a cluster\n",
    "  - [ ] NMI (Normalized Mutual Information): Tiene en cuenta la entropia dentro de cada clúster. Esto significa que si se tienen elementos que pertenencen a diferentes categorías, ya sean en `Cod_section` o `Family`, esto trae como consecuencia mayor incertidumbre, lo que se traduce a mayor información ($\\equiv\\uparrow$ entropía), lo que se traduce a menor NMI. Este escenario se le denomina clusters impuros o categorías divididas. El escenario ideal es cuando hay poca entropía y fuerte correlación entre los clústers y las categorías.\n",
    "      * Approach:\n",
    "          * El NMI evalúa cuánta información sobre las categorías reales puedes obtener conociendo los clusters, independientemente de cómo se numeran.\n",
    "          * Lo que hace el NMI es medir cuánta información te da conocer el cluster para predecir la categoría real (y viceversa).\n",
    "  - [ ] % de ruido\n",
    "  - [ ] Numero de clusters\n",
    "  - [ ] DBCV (Density-Based Clustering Validation): Estadístico diseñado especialmente para este tipo de algoritmo basados en densidad. Mide el balance entre la distancia intracluster y entre clusters, donde un mayor valor representa un mejor escenario.\n",
    "  - [ ] Homogeneity: Mide la pureza de los clusters, es decir, si sus elementos pertenencen a una misma clase (ya sea de `Section` o `Family`).\n",
    "  - [ ] Persistent score: HDBSCAN utiliza múltiples valores de `eps`. Para ello, construye una estructura jerárquica e identifica qué clústers permancen estables a través de los diferentes niveles de densidad en la jerarquía. Se espera un resultado grande, ya que esto significa que los clusters obtenidos son compactos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
